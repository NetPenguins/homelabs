---
- name: Install Kubernetes Master
  hosts: master
  become: yes
  roles: 
    - common_tasks
  tasks:
    - name: Check if control plane is already initialized
      command: kubectl get nodes --no-headers
      ignore_errors: yes
      register: control_plane_check
    - name: check whoami
      command: whoami
      register: whoami

    - name: Creating control plane and setting up kubeconfig for the user
      block:
        - shell: kubeadm init --control-plane-endpoint=192.168.50.10
          when: control_plane_check.stderr | length > 0 

    - name: Get Dashboard Token for Admin User
      shell: kubeadm token create --print-join-command
      register: master_join_command_in

    - name: Create kubeconfig directory for root user
      file:
        path: /root/.kube
        state: directory

    - name: Copy admin.conf to root user's kubeconfig
      copy:
        src: /etc/kubernetes/admin.conf
        dest: /root/.kube/config
        remote_src: yes
        owner: root
        group: root
        mode: '0600'

    - name: Create kubeconfig directory for vagrant user
      file:
        path: /home/vagrant/.kube
        state: directory
        owner: vagrant
        group: vagrant

    - name: Copy admin.conf to vagrant user's kubeconfig
      copy:
        src: /etc/kubernetes/admin.conf
        dest: /home/vagrant/.kube/config
        remote_src: yes
        owner: vagrant
        group: vagrant
        mode: '0600'

    - name: Read kubeconfig on remote host
      shell: cat /home/vagrant/.kube/config  # Adjust the path to your kubeconfig file
      register: kubeconfig_content

    - name: Save kubeconfig on localhost
      copy:
        content: "{{ kubeconfig_content.stdout }}"
        dest: ./kubeconfig.yaml  # Path to where you want to save the kubeconfig on localhost
      become: false
      delegate_to: localhost

    # Install Pod network on master node.
    # Note that this task will fail if the master node is not ready yet.
    # You may need to add a retry or wait until the master node is ready.
    # Also, you need to make sure that the master node has access to the internet.
    # If not, you need to download the calico.yaml file and change the URL to the local path.
    # You can use get_url or copy module to download or copy the file to the master node.
    # Then change the URL to the local path of the file.
    # For example, if you download or copy the file to /tmp/calico.yaml,
    # then change the URL to file:///tmp/calico.yaml.
    #
    # If you want to use a different CNI plugin, replace calico.yaml with your CNI yaml file.
    # And also replace https://docs.projectcalico.org/manifests/calico.yaml with your CNI yaml file URL or local path.
    #
    # This is just an example and may not work in your environment.
    # Please adjust it according to your environment and needs.
    - name: Install Pod network
      command: kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml

    # Task Explanation:
    # This section of the playbook focuses on setting up and configuring the Kubernetes Dashboard
    # for managing the Kubernetes cluster. 

    - name: Install Kubernetes Dashboard
      command: kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml

    - name: Create Dashboard Admin User
      block:
        - copy:
            content: |
              apiVersion: v1
              kind: ServiceAccount
              metadata:
                name: admin-user
                namespace: kubernetes-dashboard
            dest: /tmp/dashboard-adminuser.yaml
        - command: kubectl apply -f /tmp/dashboard-adminuser.yaml

    - name: Create Secret for Admin User
      block:
        - copy:
            content: |
              apiVersion: v1
              kind: Secret
              metadata:
                name: admin-user-token
                annotations:
                  kubernetes.io/service-account.name: admin-user
                namespace: kubernetes-dashboard
              type: kubernetes.io/service-account-token
            dest: /tmp/dashboard-adminuser-token.yaml
        - command: kubectl apply -f /tmp/dashboard-adminuser-token.yaml

    - name: Create ClusterRoleBinding for Admin User
      block:
        - copy:
            content: |
              apiVersion: rbac.authorization.k8s.io/v1
              kind: ClusterRoleBinding
              metadata:
                name: admin-user
              roleRef:
                apiGroup: rbac.authorization.k8s.io
                kind: ClusterRole
                name: cluster-admin
              subjects:
              - kind: ServiceAccount
                name: admin-user
                namespace: kubernetes-dashboard
            dest: /tmp/dashboard-adminuser-role.yaml
        - command: kubectl apply -f /tmp/dashboard-adminuser-role.yaml

    - name: Get Dashboard Token for Admin User
      shell: kubectl -n kubernetes-dashboard get secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user-token | awk '{print $1}') -o jsonpath="{.data.token}" | base64 --decode
      register: dashboard_token

    - debug:
        var: dashboard_token.stdout_lines

    - name: Expose Dashboard Service Externally
      command: kubectl patch svc kubernetes-dashboard -n kubernetes-dashboard --type='json' -p '[{"op":"replace","path":"/spec/type","value":"NodePort"}]'
    - name: Apply Service for dashboard nodeport
      block:
        - copy:
            content: |
              apiVersion: v1
              kind: Service
              metadata:
                name: kubernetes-dashboard
                namespace: kubernetes-dashboard
              spec:
                selector:
                  k8s-app: kubernetes-dashboard
                ports:
                - protocol: TCP
                  port: 8443
                  targetPort: 8443
                type: NodePort
            dest: /tmp/dashboard-nodeport-service.yaml
        - command: kubectl apply -f /tmp/dashboard-nodeport-service.yaml
    
    - name: Get Kubernetes Dashboard Port
      shell: kubectl get service kubernetes-dashboard -o=jsonpath='{.spec.ports[?(@.port==8443)].nodePort}' -n kubernetes-dashboard
      register: k8s_dashboard_port

    - set_fact:
        k8s_dashboard_info: 
          dashboard_host: "https://{{ ansible_host }}:{{ k8s_dashboard_port.stdout }}"
          dashboard_token: "{{dashboard_token.stdout}}"

    - debug:
        var: k8s_dashboard_info

    - name: Set shared variables
      set_fact:
        master_join_command: "{{ master_join_command_in.stdout }}"
      delegate_to: localhost

    # This set of tasks is designed to manage the existence and content of the 'join_command' variable
    # in the local shared 'vars.yml' file, which is typically used for configuration management in Ansible. 
    # It first checks if 'join_command' already exists, and based on the result, either replaces its value, 
    # creates a backup, or creates a new 'vars.yml' file with 'join_command'.

    - name: Check if join_command exists in vars.yml
      shell: grep -q '^join_command:' "{{ playbook_dir }}/vars.yml"
      register: join_command_check
      ignore_errors: yes
      become: false
      delegate_to: localhost

    - ansible.builtin.debug:
        var: join_command_check

    - name: Save join_command to vars.yml
      block:
        - name: Replace join_command in vars.yml
          replace:
            path: "{{ playbook_dir }}/vars.yml"
            regexp: '^join_command:.*'
            replace: 'join_command: {{ master_join_command }}'
          delegate_to: localhost
          become: false
          when: join_command_check.rc == 0
        - name: Backup the original vars.yml
          command: mv "{{ playbook_dir }}/vars.yml" "{{ playbook_dir }}/vars.yml.bak"
          delegate_to: localhost
          become: false
          when: join_command_check.rc == 1

        - name: Create vars.yml with join_command
          copy:
            content: |
              join_command: '{{ master_join_command }}'
            dest: "{{ playbook_dir }}/vars.yml"
          delegate_to: localhost
          become: false
          when: join_command_check.rc == 2
